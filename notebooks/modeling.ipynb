{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Model Building and Training\n",
        "\n",
        "This notebook demonstrates the complete model building and training pipeline for fraud detection, including:\n",
        "1. Data preparation with stratified train-test split\n",
        "2. Baseline model (Logistic Regression)\n",
        "3. Ensemble model (Random Forest/XGBoost/LightGBM)\n",
        "4. Model evaluation (AUC-PR, F1-Score, Confusion Matrix)\n",
        "5. Cross-validation (Stratified K-Fold)\n",
        "6. Model comparison and selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "from preprocessor import PreprocessingPipeline\n",
        "from model_pipeline import ModelPipeline\n",
        "from data_preparator import DataPreparator\n",
        "from model_trainer import ModelTrainer\n",
        "from model_evaluator import ModelEvaluator\n",
        "from cross_validator import CrossValidator\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"All modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Preprocessing\n",
        "\n",
        "First, we'll preprocess the data using the pipeline from Task 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize preprocessing pipeline\n",
        "preprocessing_pipeline = PreprocessingPipeline(\n",
        "    data_dir=\"../data/raw\",\n",
        "    output_dir=\"../data/processed\"\n",
        ")\n",
        "\n",
        "# Process fraud data (or load if already processed)\n",
        "try:\n",
        "    # Try to load processed data first\n",
        "    processed_df = pd.read_csv(\"../data/processed/processed_fraud_data.csv\")\n",
        "    print(f\"Loaded processed data: {processed_df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    # If not found, run preprocessing\n",
        "    print(\"Processed data not found. Running preprocessing pipeline...\")\n",
        "    processed_df, metadata = preprocessing_pipeline.process_fraud_data(\n",
        "        fraud_data_file=\"Fraud_Data.csv\",\n",
        "        ip_country_file=\"IpAddress_to_Country.csv\",\n",
        "        target_column=\"class\",\n",
        "        user_column=\"user_id\",\n",
        "        purchase_datetime=\"purchase_time\",\n",
        "        signup_datetime=\"signup_time\",\n",
        "        ip_column=\"ip_address\",\n",
        "        perform_eda=False,  # Skip EDA for faster processing\n",
        "        handle_imbalance=False,  # We'll handle this in model training\n",
        "        save_processed=True\n",
        "    )\n",
        "    print(f\"Preprocessing complete. Shape: {processed_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Preparation\n",
        "\n",
        "Prepare data for model training with stratified train-test split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data with stratified train-test split\n",
        "data_preparator = DataPreparator()\n",
        "\n",
        "# Determine target column name\n",
        "target_col = \"class\" if \"class\" in processed_df.columns else \"Class\"\n",
        "\n",
        "X_train, X_test, y_train, y_test = data_preparator.prepare_data(\n",
        "    processed_df,\n",
        "    target_column=target_col,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Baseline Model (Logistic Regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline Logistic Regression model\n",
        "model_trainer = ModelTrainer()\n",
        "\n",
        "baseline_model = model_trainer.train_baseline_model(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    class_weight='balanced',  # Handle class imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Baseline model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train Ensemble Model\n",
        "\n",
        "Choose one of: Random Forest, XGBoost, or LightGBM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest ensemble model\n",
        "# Alternative: Use train_xgboost() or train_lightgbm() instead\n",
        "\n",
        "ensemble_model = model_trainer.train_random_forest(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Ensemble model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Evaluate Models\n",
        "\n",
        "Evaluate both models using AUC-PR, F1-Score, and Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "model_evaluator = ModelEvaluator(output_dir=\"../models/evaluation_outputs\")\n",
        "\n",
        "# Evaluate baseline model\n",
        "baseline_results = model_evaluator.evaluate_model(\n",
        "    baseline_model,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    model_name=\"Logistic Regression\",\n",
        "    plot=True\n",
        ")\n",
        "\n",
        "print(\"\\nBaseline Model Results:\")\n",
        "print(f\"  PR-AUC: {baseline_results['pr_auc']:.4f}\")\n",
        "print(f\"  F1-Score: {baseline_results['f1_score']:.4f}\")\n",
        "print(f\"  ROC-AUC: {baseline_results['roc_auc']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate ensemble model\n",
        "ensemble_results = model_evaluator.evaluate_model(\n",
        "    ensemble_model,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    model_name=\"Random Forest\",\n",
        "    plot=True\n",
        ")\n",
        "\n",
        "print(\"\\nEnsemble Model Results:\")\n",
        "print(f\"  PR-AUC: {ensemble_results['pr_auc']:.4f}\")\n",
        "print(f\"  F1-Score: {ensemble_results['f1_score']:.4f}\")\n",
        "print(f\"  ROC-AUC: {ensemble_results['roc_auc']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Cross-Validation (Stratified K-Fold)\n",
        "\n",
        "Perform 5-fold cross-validation for reliable performance estimation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform cross-validation\n",
        "cross_validator = CrossValidator(n_splits=5, random_state=42)\n",
        "\n",
        "# Cross-validate baseline\n",
        "baseline_cv = cross_validator.cross_validate(\n",
        "    baseline_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    model_name=\"Logistic Regression\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validate ensemble\n",
        "ensemble_cv = cross_validator.cross_validate(\n",
        "    ensemble_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    model_name=\"Random Forest\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Model Comparison and Selection\n",
        "\n",
        "Compare all models side-by-side and select the best model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "comparison_results = {\n",
        "    'Logistic Regression': baseline_results,\n",
        "    'Random Forest': ensemble_results\n",
        "}\n",
        "\n",
        "comparison_df = model_evaluator.compare_models(\n",
        "    comparison_results,\n",
        "    output_file=\"model_comparison.csv\"\n",
        ")\n",
        "\n",
        "display(comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Select Best Model\n",
        "\n",
        "Select the best model based on PR-AUC (most important for imbalanced data) and interpretability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model_metrics = comparison_results[best_model_name]\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"\\nJustification:\")\n",
        "print(f\"  - PR-AUC: {best_model_metrics['pr_auc']:.4f} (primary metric for imbalanced data)\")\n",
        "print(f\"  - F1-Score: {best_model_metrics['f1_score']:.4f}\")\n",
        "print(f\"  - Precision: {best_model_metrics['precision']:.4f}\")\n",
        "print(f\"  - Recall: {best_model_metrics['recall']:.4f}\")\n",
        "\n",
        "# Save best model\n",
        "if best_model_name == \"Logistic Regression\":\n",
        "    best_model = baseline_model\n",
        "else:\n",
        "    best_model = ensemble_model\n",
        "\n",
        "model_trainer.save_model(\n",
        "    best_model_name.lower().replace(\" \", \"_\"),\n",
        "    f\"../models/{best_model_name.lower().replace(' ', '_')}_best_model.joblib\"\n",
        ")\n",
        "\n",
        "print(f\"\\nBest model saved to models/ directory\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
