{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Model Explainability with SHAP\n",
        "\n",
        "This notebook demonstrates complete model explainability analysis using SHAP, including:\n",
        "1. Built-in feature importance extraction\n",
        "2. SHAP summary plot (global feature importance)\n",
        "3. SHAP force plots for individual predictions (TP, FP, FN)\n",
        "4. Comparison of SHAP vs built-in importance\n",
        "5. Top 5 fraud prediction drivers\n",
        "6. Business recommendations based on insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "from explainability_pipeline import ExplainabilityPipeline\n",
        "from model_trainer import ModelTrainer\n",
        "from model_evaluator import ModelEvaluator\n",
        "from data_preparator import DataPreparator\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"All modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Data and Model\n",
        "\n",
        "Load preprocessed data and trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "try:\n",
        "    processed_df = pd.read_csv(\"../data/processed/processed_fraud_data.csv\")\n",
        "    print(f\"Loaded processed data: {processed_df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Processed data not found. Please run preprocessing pipeline first.\")\n",
        "    processed_df = None\n",
        "\n",
        "# Load trained model (or train one if not available)\n",
        "try:\n",
        "    model_trainer = ModelTrainer()\n",
        "    # Try to load best model\n",
        "    best_model = model_trainer.load_model(\n",
        "        \"best_model\",\n",
        "        \"../models/random_forest_best_model.joblib\"\n",
        "    )\n",
        "    print(\"Loaded trained model\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Trained model not found. Training a model first...\")\n",
        "    if processed_df is not None:\n",
        "        # Prepare data and train model\n",
        "        data_prep = DataPreparator()\n",
        "        target_col = \"class\" if \"class\" in processed_df.columns else \"Class\"\n",
        "        X_train, X_test, y_train, y_test = data_prep.prepare_data(processed_df, target_col)\n",
        "        \n",
        "        model_trainer = ModelTrainer()\n",
        "        best_model = model_trainer.train_random_forest(\n",
        "            X_train, y_train,\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        )\n",
        "        print(\"Model trained successfully\")\n",
        "    else:\n",
        "        best_model = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize Explainability Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize explainability pipeline\n",
        "explainability_pipeline = ExplainabilityPipeline(\n",
        "    output_dir=\"../models/explainability_outputs\"\n",
        ")\n",
        "\n",
        "print(\"Explainability pipeline initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Complete Explainability Analysis\n",
        "\n",
        "Run the complete explainability pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if processed_df is not None and best_model is not None:\n",
        "    # Prepare data\n",
        "    data_prep = DataPreparator()\n",
        "    target_col = \"class\" if \"class\" in processed_df.columns else \"Class\"\n",
        "    X_train, X_test, y_train, y_test = data_prep.prepare_data(processed_df, target_col)\n",
        "    \n",
        "    # Run complete explainability analysis\n",
        "    results = explainability_pipeline.explain_model(\n",
        "        model=best_model,\n",
        "        X_train=X_train,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        model_name=\"Random Forest\",\n",
        "        sample_size=1000\n",
        "    )\n",
        "    \n",
        "    print(\"\\nExplainability analysis complete!\")\n",
        "    print(f\"Top 5 drivers identified: {len(results['top_drivers'])}\")\n",
        "    print(f\"Force plots generated: {len(results['force_plots'])}\")\n",
        "    print(f\"Recommendations: {len(results['recommendations'])}\")\n",
        "else:\n",
        "    print(\"Cannot run explainability analysis. Please ensure data and model are available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Feature Importance Comparison\n",
        "\n",
        "Compare built-in and SHAP feature importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'results' in locals() and results['comparison'] is not None:\n",
        "    print(\"Feature Importance Comparison:\")\n",
        "    print(\"=\" * 80)\n",
        "    display(results['comparison'])\n",
        "    \n",
        "    print(\"\\nKey Insights:\")\n",
        "    print(f\"  - Built-in importance top feature: {results['builtin_importance'].iloc[0]['feature']}\")\n",
        "    print(f\"  - SHAP importance top feature: {results['shap_importance'].iloc[0]['feature']}\")\n",
        "    \n",
        "    # Check for differences\n",
        "    builtin_top5 = set(results['builtin_importance'].head(5)['feature'])\n",
        "    shap_top5 = set(results['shap_importance'].head(5)['feature'])\n",
        "    common = builtin_top5 & shap_top5\n",
        "    \n",
        "    print(f\"  - Common top 5 features: {len(common)}\")\n",
        "    if len(common) < 3:\n",
        "        print(\"  - ⚠️ Significant difference between built-in and SHAP importance!\")\n",
        "    else:\n",
        "        print(\"  - ✓ Good agreement between built-in and SHAP importance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Top 5 Fraud Prediction Drivers\n",
        "\n",
        "Analyze the top drivers of fraud predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'results' in locals() and results['top_drivers']:\n",
        "    print(\"Top 5 Fraud Prediction Drivers:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for i, driver in enumerate(results['top_drivers'], 1):\n",
        "        print(f\"\\n{i}. {driver['feature']}\")\n",
        "        print(f\"   Category: {driver['category']}\")\n",
        "        print(f\"   SHAP Importance: {driver['importance']:.4f}\")\n",
        "        print(f\"   Interpretation: {driver['interpretation']}\")\n",
        "    \n",
        "    # Create summary dataframe\n",
        "    drivers_df = pd.DataFrame(results['top_drivers'])\n",
        "    display(drivers_df[['rank', 'feature', 'category', 'importance']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Individual Prediction Analysis\n",
        "\n",
        "Analyze specific cases: True Positive, False Positive, False Negative\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'results' in locals() and results['force_plots']:\n",
        "    print(\"Individual Prediction Analysis:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for plot_info in results['force_plots']:\n",
        "        print(f\"\\n{plot_info['prediction_type'].upper()} Case (Index: {plot_info['instance_idx']}):\")\n",
        "        print(f\"  Predicted: {plot_info['predicted']}, Actual: {plot_info['actual']}\")\n",
        "        print(f\"  Fraud Probability: {plot_info['probability']:.4f}\")\n",
        "        \n",
        "        # Show top contributing features\n",
        "        shap_vals = plot_info['shap_values']\n",
        "        feature_vals = plot_info['feature_values']\n",
        "        \n",
        "        # Get top 5 contributing features\n",
        "        feature_names = list(feature_vals.keys())\n",
        "        top_contrib_idx = np.argsort(np.abs(shap_vals))[-5:][::-1]\n",
        "        \n",
        "        print(f\"\\n  Top 5 Contributing Features:\")\n",
        "        for idx in top_contrib_idx:\n",
        "            feature = feature_names[idx]\n",
        "            shap_val = shap_vals[idx]\n",
        "            feat_val = feature_vals[feature]\n",
        "            direction = \"increases\" if shap_val > 0 else \"decreases\"\n",
        "            print(f\"    - {feature}: {shap_val:.4f} ({direction} fraud probability)\")\n",
        "            print(f\"      Feature value: {feat_val}\")\n",
        "    \n",
        "    print(\"\\n✓ Force plots saved to models/explainability_outputs/\")\n",
        "    print(\"  View the PNG files for detailed visualizations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Business Recommendations\n",
        "\n",
        "Review actionable business recommendations based on SHAP insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'results' in locals() and results['recommendations']:\n",
        "    print(\"Business Recommendations Based on SHAP Analysis:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for i, rec in enumerate(results['recommendations'], 1):\n",
        "        print(f\"\\nRecommendation {i}: {rec['title']}\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{rec['recommendation']}\")\n",
        "        print(f\"\\nAction: {rec['action']}\")\n",
        "        print(f\"Expected Impact: {rec['expected_impact']}\")\n",
        "        print(f\"SHAP Insight: {rec['shap_insight']}\")\n",
        "        print()\n",
        "    \n",
        "    print(\"\\n✓ Full recommendations saved to models/explainability_outputs/business_recommendations.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This analysis provides:\n",
        "- ✅ Built-in feature importance visualization\n",
        "- ✅ SHAP summary plot (global feature importance)\n",
        "- ✅ SHAP force plots for TP, FP, FN cases\n",
        "- ✅ Comparison of built-in vs SHAP importance\n",
        "- ✅ Top 5 fraud prediction drivers identified\n",
        "- ✅ Actionable business recommendations with SHAP justification\n",
        "\n",
        "All outputs are saved to `models/explainability_outputs/` directory.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
