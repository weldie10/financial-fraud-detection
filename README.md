# Financial Fraud Detection

A comprehensive machine learning project for detecting financial fraud using classification algorithms. Implements complete preprocessing and model training pipelines with OOP-based modules following industry best practices.

## Project Structure

```
financial-fraud-detection/
├── data/                  # Data directory (see data/README.md)
│   ├── raw/                # Original, immutable source datasets
│   └── processed/          # Cleaned and transformed datasets
├── src/                    # Production source code modules
│   ├── data_loader.py           # Data loading & validation
│   ├── data_cleaner.py          # Data cleaning
│   ├── geolocation.py           # IP to country mapping
│   ├── feature_engineer.py      # Feature engineering
│   ├── eda.py                   # Exploratory data analysis
│   ├── data_transformer.py      # Scaling & encoding
│   ├── imbalance_handler.py     # SMOTE/undersampling
│   ├── preprocessor.py          # Preprocessing pipeline
│   ├── data_preparator.py       # Train-test split
│   ├── model_trainer.py         # Model training
│   ├── model_evaluator.py       # Model evaluation
│   ├── cross_validator.py       # Cross-validation
│   ├── hyperparameter_tuner.py  # Hyperparameter tuning
│   ├── model_pipeline.py        # Complete model pipeline
│   ├── model_explainer.py       # SHAP explainability
│   ├── business_recommender.py   # Business recommendations
│   └── explainability_pipeline.py # Complete explainability pipeline
├── notebooks/              # Interactive Jupyter notebooks
│   ├── eda-fraud-data.ipynb
│   ├── eda-creditcard.ipynb
│   ├── feature-engineering.ipynb
│   ├── modeling.ipynb           # Task 2: Model building
│   └── shap-explainability.ipynb # Task 3: Model explainability
├── models/                 # Model artifacts (see models/README.md)
│   ├── *.joblib            # Saved trained models
│   └── evaluation_outputs/ # Evaluation visualizations
├── reports/                 # Project reports (see reports/README.md)
│   └── INTERIM_REPORT_TASK1.md
├── tests/                   # Test suite (see tests/README.md)
│   ├── unit/               # Unit tests
│   └── integration/        # Integration tests
├── scripts/                 # Utility scripts
│   └── generate_visualizations.py
└── requirements.txt         # Python dependencies
```

## Directory Usage

### `data/` - Data Management

**Purpose**: Store all datasets used in the project.

- **`data/raw/`**: Original, immutable source datasets. Never modify files here.
  - Place source CSV files: `Fraud_Data.csv`, `creditcard.csv`, `IpAddress_to_Country.csv`
- **`data/processed/`**: Cleaned and transformed datasets generated by preprocessing pipeline.
  - Contains: processed CSVs, EDA visualizations, saved transformers

**See**: `data/README.md` for detailed guidelines.

### `src/` - Source Code

**Purpose**: Production-ready Python modules implementing core functionality.

- Modular OOP design with single responsibility principle
- Comprehensive error handling and logging
- Reusable classes for data processing and model training
- All modules can be imported and used independently

### `notebooks/` - Interactive Analysis

**Purpose**: Jupyter notebooks for exploratory analysis and demonstrations.

- Use for interactive data exploration
- Document analysis workflows
- Share findings and visualizations
- Not for production code (use `src/` instead)

### `models/` - Model Artifacts

**Purpose**: Store trained models and evaluation outputs.

- **Saved Models**: Serialized model files (`.joblib` format)
- **Configurations**: Model hyperparameters and settings
- **Evaluations**: Confusion matrices, PR curves, ROC curves, comparison tables

**See**: `models/README.md` for usage details.

### `reports/` - Documentation

**Purpose**: Project reports and documentation.

- Interim and final reports
- Methodology documentation
- Business recommendations
- Technical specifications

**See**: `reports/README.md` for contents.

### `tests/` - Test Suite

**Purpose**: Unit and integration tests for code quality assurance.

- **`tests/unit/`**: Test individual modules and functions in isolation
- **`tests/integration/`**: Test complete pipelines and workflows

**See**: `tests/README.md` for testing guidelines.

### `scripts/` - Utility Scripts

**Purpose**: Standalone utility scripts for automation.

- Data generation scripts
- Visualization generation
- Batch processing utilities

## Quick Start

### 1. Setup

```bash
# Clone repository
git clone <repository-url>
cd financial-fraud-detection

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Prepare Data

Place your datasets in `data/raw/`:
- `Fraud_Data.csv` - E-commerce fraud data
- `creditcard.csv` - Credit card fraud data
- `IpAddress_to_Country.csv` - IP to country mapping (optional)

### 3. Run Preprocessing (Task 1)

```python
from src.preprocessor import PreprocessingPipeline

pipeline = PreprocessingPipeline()
processed_df, metadata = pipeline.process_fraud_data(
    fraud_data_file="Fraud_Data.csv",
    perform_eda=True,
    handle_imbalance=True
)
```

### 4. Train Models (Task 2)

```python
from src.model_pipeline import ModelPipeline

model_pipeline = ModelPipeline()
results = model_pipeline.build_and_evaluate_models(
    df=processed_df,
    target_column="class",
    perform_cv=True,
    ensemble_model="random_forest"
)
```

### 5. Explain Model (Task 3)

```python
from src.explainability_pipeline import ExplainabilityPipeline

explainability = ExplainabilityPipeline()
results = explainability.explain_model(
    model=best_model,
    X_train=X_train,
    X_test=X_test,
    y_test=y_test,
    model_name="Random Forest"
)

# Access recommendations
for rec in results['recommendations']:
    print(f"{rec['title']}: {rec['recommendation']}")
```

## Testing

### Run All Tests

```bash
# From project root
pytest tests/ -v
```

### Run Unit Tests Only

```bash
pytest tests/unit/ -v
```

### Run Integration Tests Only

```bash
pytest tests/integration/ -v
```

### Run with Coverage Report

```bash
# Generate coverage report
pytest tests/ --cov=src --cov-report=html --cov-report=term

# View HTML report
open htmlcov/index.html  # Mac/Linux
# or
start htmlcov/index.html  # Windows
```

### End-to-End Testing

To test the complete pipeline end-to-end:

```bash
# 1. Run unit tests
pytest tests/unit/ -v

# 2. Run integration tests (requires data files)
pytest tests/integration/ -v

# 3. Check coverage
pytest tests/ --cov=src --cov-report=term-missing

# 4. Run with verbose output
pytest tests/ -v --tb=short
```

**Note**: Integration tests may require data files in `data/raw/`. See individual test files for requirements.

## Features

### Task 1: Data Preprocessing
- ✅ Data loading with validation
- ✅ Missing value imputation & duplicate removal
- ✅ IP to country geolocation mapping
- ✅ Feature engineering (temporal, velocity, frequency)
- ✅ Data transformation (scaling, encoding)
- ✅ Class imbalance handling (SMOTE, undersampling)
- ✅ Comprehensive EDA with visualizations

### Task 2: Model Building & Training
- ✅ Stratified train-test split (preserves class distribution)
- ✅ Baseline model: Logistic Regression with class weights
- ✅ Ensemble models: Random Forest, XGBoost, LightGBM
- ✅ Model evaluation: AUC-PR, F1-Score, ROC-AUC, Confusion Matrix
- ✅ Cross-validation: Stratified K-Fold (k=5)
- ✅ Hyperparameter tuning with RandomizedSearchCV
- ✅ Model comparison and selection
- ✅ Model persistence (save/load)

### Task 3: Model Explainability
- ✅ Built-in feature importance extraction and visualization
- ✅ SHAP summary plot (global feature importance)
- ✅ SHAP force plots for individual predictions (TP, FP, FN)
- ✅ Feature importance comparison (built-in vs SHAP)
- ✅ Top 5 fraud prediction drivers identification
- ✅ Individual prediction analysis with detailed explanations
- ✅ Business recommendations generator with SHAP justification
- ✅ Automatic case finding (True Positive, False Positive, False Negative)

## Datasets

### Credit Card Fraud Detection
- **Source:** [Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) | [Zenodo](https://zenodo.org/record/7395559)
- **Size:** 284,807 transactions (492 fraudulent, 0.17%)
- **Features:** Time, V1-V28 (PCA), Amount, Class

### Fraud Data (E-commerce)
- **Expected columns:** user_id, signup_time, purchase_time, purchase_value, ip_address, class
- **Note:** Custom dataset structure

### IP to Country Mapping
- **Sources:** MaxMind GeoIP2, IP2Location, DB-IP
- **Format:** lower_bound_ip_address, upper_bound_ip_address, country

## Usage Examples

### Complete Pipeline

```python
# 1. Preprocess data
from src.preprocessor import PreprocessingPipeline
preprocessor = PreprocessingPipeline()
df, _ = preprocessor.process_fraud_data("Fraud_Data.csv")

# 2. Train and evaluate models
from src.model_pipeline import ModelPipeline
modeler = ModelPipeline()
results = modeler.build_and_evaluate_models(df, target_column="class")

# 3. Access best model
best_model = results['best_model']['model']

# 4. Explain model predictions
from src.explainability_pipeline import ExplainabilityPipeline
explainability = ExplainabilityPipeline()
explain_results = explainability.explain_model(
    model=best_model,
    X_train=X_train,
    X_test=X_test,
    y_test=y_test
)
```

### Individual Modules

```python
from src.data_preparator import DataPreparator
from src.model_trainer import ModelTrainer
from src.model_evaluator import ModelEvaluator

# Prepare data
prep = DataPreparator()
X_train, X_test, y_train, y_test = prep.prepare_data(df, "class")

# Train model
trainer = ModelTrainer()
model = trainer.train_random_forest(X_train, y_train)

# Evaluate
evaluator = ModelEvaluator()
metrics = evaluator.evaluate_model(model, X_test, y_test)
```

## Notebooks

- **`notebooks/modeling.ipynb`** - Complete model building pipeline (Task 2)
- **`notebooks/shap-explainability.ipynb`** - Model explainability with SHAP (Task 3)
- **`notebooks/eda-fraud-data.ipynb`** - EDA for fraud data
- **`notebooks/feature-engineering.ipynb`** - Feature engineering examples

## Requirements

Key dependencies:
- pandas, numpy - Data manipulation
- scikit-learn - Machine learning
- xgboost, lightgbm - Ensemble models
- imbalanced-learn - Class imbalance handling
- matplotlib, seaborn - Visualization
- shap - Model explainability (Task 3)
- pytest, pytest-cov - Testing

See `requirements.txt` for complete list.

## Project Status

- ✅ **Task 1:** Data preprocessing pipeline complete
- ✅ **Task 2:** Model building and training complete
- ✅ **Task 3:** Model explainability (SHAP) complete

## Contributing

1. Create a feature branch
2. Write tests for new functionality
3. Ensure all tests pass: `pytest tests/ -v`
4. Submit a pull request

## License

TBD
